{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15781498-7cf6-4426-bb4d-b5c471c13829",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment 7: 75 points \n",
    "## Neural Networks: Scikit-Learn and Keras\n",
    "\n",
    "### IMPORTANT: \n",
    "\n",
    "You MUST read everything in tnis notebook CAREFULLY, including ALL code comments.  If you do not, then you may easily make mistakes.\n",
    "\n",
    "Be sure to review the class slides if you need to. (But read the comments in this notebook first.)\n",
    "\n",
    "Relevant documentation for sklearn's generic neural network is here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "\n",
    "Keras API Topic Reference is here:\n",
    "\n",
    "https://keras.io/api/\n",
    "\n",
    "### Heads up: Some of these models may take some time to run, so use playsound and make productive use of your time while you are waiting for them to finish training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db9ab79-03f6-47a0-a8a6-d719dbf81685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: 5 points.  Set up environment\n",
    "\n",
    "####################################################################################\n",
    "# If some of these do not import properly, you may need to install them and re-run #\n",
    "####################################################################################\n",
    "\n",
    "import playsound\n",
    "import sklearn\n",
    "import tensorflow\n",
    "from   tensorflow import keras\n",
    "import time\n",
    "    \n",
    "import matplotlib         as mpl   \n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy              as np   \n",
    "import pandas             as pd\n",
    "\n",
    "from keras.datasets          import cifar10  \n",
    "from playsound               import playsound\n",
    "from pprint                  import pprint   \n",
    "\n",
    "from sklearn.cluster         import KMeans\n",
    "from sklearn.decomposition   import PCA\n",
    "from sklearn.ensemble        import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model    import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics         import confusion_matrix, precision_recall_curve, precision_score, recall_score, f1_score, silhouette_score, homogeneity_score, completeness_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\n",
    "from sklearn.neural_network  import MLPClassifier\n",
    "from sklearn.pipeline        import make_pipeline\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.svm             import LinearSVC, SVC\n",
    "from sklearn.tree            import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "from yellowbrick.classifier  import ClassBalance, ClassificationReport, ClassPredictionError, ConfusionMatrix\n",
    "from yellowbrick.cluster     import SilhouetteVisualizer\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7db16-b0e3-4ae2-874c-0d48b8322720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here is the code to load and prep the CIFAR-10 data\n",
    "\n",
    "# Notice that there are two new additional lines for some\n",
    "# new variables that will be used for early stopping\n",
    "\n",
    "np.random.seed(42) # Make this notebook's output stable across runs\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "\n",
    "# Normalize the data\n",
    "X_train  = X_train.astype('float32')\n",
    "X_test   = X_test.astype('float32')\n",
    "X_train /= 255.0  # The largest number is 255, and the smallest 0\n",
    "X_test  /= 255.0  # So this division will normalize the data.\n",
    "\n",
    "################## Here are the two new lines I mentioned above ################################\n",
    "X_valid, X_train_ = X_train[:5000], X_train[5000:] # 1st 5000 for validation in early stopping #\n",
    "y_valid, y_train_ = y_train[:5000], y_train[5000:] # X_train_ and y_train_ for early stopping  #\n",
    "################################################################################################\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]*X_train.shape[3])\n",
    "X_test_flat  = X_test.reshape(X_test.shape[0],   X_test.shape[1]*X_test.shape[2]*X_test.shape[3])\n",
    "\n",
    "# We also have to use ravel to change the target values (the values we want to predict). \n",
    "# Notice that ALL FOUR of the target value variables need this.\n",
    "y_train  = np.ravel(y_train)\n",
    "y_train_ = np.ravel(y_train_) # This is NOT the same variable as the preceding line.\n",
    "y_valid  = np.ravel(y_valid)\n",
    "y_test   = np.ravel(y_test)\n",
    "\n",
    "LABEL_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "'Done' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1b034-4c67-41de-8569-bc46874a91f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### We will begin with Sklearn's Implementation of Neural Nets\n",
    "\n",
    "Even though sklearn's API for fully-connected, feed-forward neural nets (FFNN) has numerous hyperparameters, they only give you very limited control over your network.  Nonetheless, let's start here and then we will do a comparison with Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4febfc25-20de-427d-b133-953eac522e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 2: 10 points \n",
    "\n",
    "# Generic FFNN using sklearn\n",
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# First, let's start with the simplest possible architecture of a neural net\n",
    "# with only one hidden layer so we can establish a baseline.  \n",
    "\n",
    "# Build a FFNN with a single hidden layer using the MLPClassifier class.\n",
    "# \"MLP\" means \"Multi-Layer Perceptron\" which is sometimes used \n",
    "# to refer to a FFNN, which is not really a Perceptron in the original sense.\n",
    "\n",
    "# To avoid waiting a long time for your model to train, set it to a maximum of \n",
    "# 30 iterations.  \n",
    "# Consult sklearn's documentation (see the URL at the top of the notebook)\n",
    "# for the exact name of this parameter.\n",
    "# I also suggest setting the 'verbose' parameter to 'True'\n",
    "# (without the quotes) so you can watch its progress and anticipate when\n",
    "# it will finish.\n",
    "\n",
    "# 4 points: call MLPClassifier with the two parameter settings I just mentioned.\n",
    "#           and save the network into variable sklMLP\n",
    "# 4 points: call the fit method of sklMLP and pass it the training data, \n",
    "#           as you have done many times now for different models.\n",
    "# 1 point:  Add a print statement with an appropriate message to show\n",
    "#           the accuracy score on the test data, as you have also done\n",
    "#           many times by now.\n",
    "# 1 point:  Add all the code you need to capture the starting and stopping\n",
    "#           time, and print an appropriate timing message.\n",
    "\n",
    "####################  insert your code below for 10 points ####################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sklMLP    = \n",
    "\n",
    "\n",
    "              \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# 26s, 0.3876 on my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7669ff-b807-45cf-b071-a31a7711f4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 3: 10 points: \n",
    "\n",
    "# Let's define and train an equivalent model in keras \n",
    "# and see how it does compared to the sklearn model\n",
    "\n",
    "# By default, the Dense layer in Keras uses no activation function, i.e.\n",
    "# it's equivalent to the identify function and just passes on the result\n",
    "# of its dot product input!  Thus, we have to explicity tell it to use\n",
    "# 'relu' which is the default used in sklearn.\n",
    "# Also, while sklearn has a default of 100 neurons for the hidden layer,\n",
    "# Keras requires that we explicitly give the neuron count as the FIRST\n",
    "# argument to any Dense layer.\n",
    "\n",
    "# You can consult Géron's section 'Creating the model using the Sequential API'\n",
    "# in the textbook (or just look at the Chapter 10 jupyter notebook by Géron)\n",
    "# for an example of building a Sequential Keras model.\n",
    "# I recommend the second version shown in the book (just following the 'TIP' \n",
    "# box) for its simplicity, but either version is fine for this assignment.\n",
    "\n",
    "# Create a Sequential Keras model with 3 layers as follows:\n",
    "# 2 points: Define the input layer using Flatten, and set the appropriate\n",
    "#           parameter to the input shape of the CIFAR10 images, i.e. [32, 32, 3]\n",
    "#           Notice that this input layer will perform the flattening, so you\n",
    "#           should NOT train (below) it with the flattened input vectors \n",
    "#           but with the natural shape of the training data.\n",
    "# 2 points: Define the Dense hidden layer with 100 neurons and the 'relu' \n",
    "#           activation function so we have a model comparable to the previous cell\n",
    "# 2 points: Define the Dense output layer for the 10 output neurons, 1 for each\n",
    "#           of the 10 categories, and using 'softmax' for the activation function\n",
    "#           Save this Sequential model definition into variable kerasModel1\n",
    "\n",
    "# Compile kerasModel1: \n",
    "# 2 points: Use 'sparse_categorical_crossentropy' as the loss function,\n",
    "#           the 'Adam' optimizer (again to match the previous cell)\n",
    "#           and use 'accuracy' for the value of metrics\n",
    "\n",
    "# 1 point:  Call the summary method on kerasModel1\n",
    "\n",
    "# Fit kerasModel1: \n",
    "# 1 point:  Call the 'fit' method of kerasModel1 on the training data,\n",
    "#           and set it to run for 30 epochs.\n",
    "\n",
    "startTime   = time.perf_counter() \n",
    "\n",
    "####################  insert your code below for 10 points ####################\n",
    "\n",
    "# Define Model\n",
    "kerasModel1 = \n",
    "\n",
    "# Compile Model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "kerasHistory1 = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### Your code ends above ##############################\n",
    "\n",
    "stopTime      = time.perf_counter()                                      \n",
    "print(f'\\nElapsed time for kerasModel1: {stopTime - startTime:0.0f} seconds') \n",
    "print('Accuracy of FFNN in Keras:', kerasModel1.evaluate(X_test, y_test))\n",
    "\n",
    "# 259 seconds on my computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b339ad-a4fe-4f7a-b7f4-50d572d4f68d",
   "metadata": {},
   "source": [
    "#### Did you see any interesting differences between sklearn and keras models?\n",
    "\n",
    "sklearn has 2 clear advantages over keras: (using results from my computer and more than one run)\n",
    "1.  Between 8x and 10x faster with sklearn.\n",
    "2. A fraction of the code that is needed for Keras\n",
    "\n",
    "I have run this more than once, sometimes Keras had an accuracy advantage and sometimes not, so it may seem you should always use sklearn for neural nets, right?  Not really. The vast majority of practitioners use either Keras/TensorFlow or PyTorch. So why does hardly anyone use sklearn?\n",
    "\n",
    "If you look at the documentation for sklearn vs. keras you will notice that you cannot control very much with the current implementation of sklearn's MLPClassifier vis-a-vis Keras.  In sklearn, you only have 4 activation functions to choose from, and only 3 optimizers, for example.  Further, in sklearn there are NO options to use dropout, different weight initialization strategies, batch normalization, custom architectures, and much, much more.  (All of these concepts are covered in our class in the final weeks, in case you don't yet know what they are.)\n",
    "\n",
    "The takeaway from these facts are:  If you only need a 'standard' FFNN neural net, then sklearn may be an excellent choice.  But if you need to explore more options to tune your model and achieve the best possible performance, then don't use sklearn.  Use Keras/TensorFlow or PyTorch.\n",
    "\n",
    "That said, I do expect more options to become available in sklearn in future releases.  But the same can be said of Keras, which will almost certainly be ahead of sklearn at any point in time because of its vast, and influential user base. \n",
    "\n",
    "But given that the above models did not even achieve the accuracy of our best non-neural network models, what can we do to get higher accuracy with a neural net?  Our best previous model is still only slightly better than 51%, which is not very good at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a6992-d4a4-4a22-a9d0-f5ea36493375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 4: 10 points\n",
    "\n",
    "# To try and do better with a neural network, let's first\n",
    "# begin by building more powerful models.  How about simply\n",
    "# adding more hidden layers, and perhaps use more than 100\n",
    "# neurons?  Both should help us identify features in color\n",
    "# images more effectively.  \n",
    "\n",
    "# Since we will still use a fully-connected, feed-forward net\n",
    "# we may as well use sklearn again for the time savings.\n",
    "\n",
    "# Consult the sklearn MLPClassifier documentation (URL at the top)\n",
    "# for the parameter names to build the following:\n",
    "\n",
    "# 1 point: 3 hidden layers with this many neurons for each:\n",
    "#            (500, 1000, 2000)\n",
    "#   (Naturally, you need to also add the input and output layers)\n",
    "# 1 point: maximum iterations of 30\n",
    "# 1 point: verbose setting of True\n",
    "# 1 point: This time we will set the early stopping parameter\n",
    "#            to True, which may halt the training before it\n",
    "#            performs all 30 iterations.\n",
    "# 1 point: Set the number of iterations with no change to 5,\n",
    "#            which will trigger early stopping if there are\n",
    "#            5 consecutive iterations with no substantial\n",
    "#            improvement.\n",
    "#            Set the random state to 42\n",
    "#  Save this neural net in variable mlp\n",
    "\n",
    "# 1 point: Call the fit method of mlp on the training data\n",
    "# 1 point: Capture the start and stopping time\n",
    "# 1 point: Print the elapsed time with an appropriate message\n",
    "# 1 point: Print the test data accuracy with an appropriate message\n",
    "# 1 point: Call \n",
    "\n",
    "####################  insert your code below for 10 points ####################\n",
    "\n",
    "\n",
    "\n",
    "mlp = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### Your code ends above ##############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c66221-8c1d-489a-80dc-668ed7efc503",
   "metadata": {},
   "source": [
    "On my computer this model took 318 seconds (more than 5 minutes), automatically stopping early after only 20 iterations.  While it did get significantly better results (51.6%) by using more neurons and 2 additional hidden layers, it's still about the same as what we have previously seen with other ML algorithms.  When I trained a model with 4 hidden layers having 1000, 2000, 3000 and 4000 neurons I got 54%. While that is the best so far, and somewhat better than the previous cell, it's still not very good, and it's a LONG wait, but feel free to try it yourself if you are curious and have lots of free time!\n",
    "\n",
    "One idea that we can try for better accuracy is changing the activation function that we have been using.  So instead of 'relu' let's try one of its variants, called 'ELU', which generally gives better results.  But first, let's also set up the early stopping mechanism for Keras models, which is  more complex than what you get with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423b941-8fcc-4803-9721-cf3514f84ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: 5 points \n",
    "\n",
    "# Define the Early Stopping Callback in Keras\n",
    "\n",
    "# You can find an example of this in the section 'Using Callbacks'\n",
    "# of Chapter 10, or in the notebooks for chs. 10 or 11.\n",
    "\n",
    "# PLEASE NOTE: By using early stopping, you will be diverted 10% of your\n",
    "# training data to be used for validation to trigger early stopping.\n",
    "# Any model built with 10% less training data will almost certainly suffer\n",
    "# a small loss in predictive accuracy.  However, just like you use reduced\n",
    "# training data when using cross-validation, once you have found your best\n",
    "# model algorithm and architecture, you can always train it again (i.e. \n",
    "# without early stopping) using ALL the training data for your final model.  \n",
    "\n",
    "# 5 points: define an early stopping callback with a patience of 5, and set\n",
    "#           to restore the best weights when done.\n",
    "#           Save the callback into variable early_stopping_cb\n",
    "# NOTE: If you ignore the comments in the rest of this cell, then you won't\n",
    "#       know how to use this.  So keep reading.\n",
    "\n",
    "####################  insert your code below for 5 points ####################\n",
    "\n",
    "early_stopping_cb = \n",
    "\n",
    "\n",
    "\n",
    "########################### Your code ends above ##############################\n",
    "\n",
    "# Below (but commented out so it does not actually execute in this cell \n",
    "# is some template code to use when you fit a Keras model with early stopping\n",
    "# in the rest of this notebook.\n",
    "\n",
    "# Replace DUMMYMODEL with the actual name of a Kera model in cells below.\n",
    "# When using early stopping, remember NOT to use X_train and y_train\n",
    "# Instead, use X_train_ and y_train_  with the extra '_' at the end, \n",
    "# and you also need to use the rest of the code below as written.\n",
    "\n",
    "# You may remember that I defined X_train_ and y_train_ for you \n",
    "# above in cell 2 of this notebook.\n",
    "\n",
    "# I am setting epochs=30 here, so training will go no longer than 30, \n",
    "# epochs just in case early stopping does not get triggered.\n",
    "\n",
    "# After you run this cell, just remove the triple tick-mark comment characters\n",
    "# below.  Then copy the code, and paste it into your other models in the rest \n",
    "# of this notebook when you'd like to use early stopping to potentially \n",
    "# reduce your waiting time while models are training.  Be sure to replace \n",
    "# both occurrences of DUMMYMODEL when you use it.\n",
    "\n",
    "'''\n",
    "DUMMYMODEL_History  = DUMMYMODEL.fit(X_train_, y_train_, \n",
    "                                     epochs = 30,         \n",
    "                                     validation_data = (X_valid, y_valid),\n",
    "                                     callbacks = [early_stopping_cb]) \n",
    "'''\n",
    "\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882dde5-d8e9-4ffd-aeb7-4c927897514c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 6: 10 points\n",
    "\n",
    "# ELU: Exponential Linear Unit\n",
    "# Géron points out that ELU often (perhaps usually) performs better than\n",
    "# ReLU and other variants, so let's give it a try. \n",
    "\n",
    "tensorflow.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# I have already tested this out on keras architecture like the \n",
    "# last sklearn model we did above with 3 layers of 500, 1000, 2000 \n",
    "# neurons. In Keras, that model has 4,059,510 parameters, and \n",
    "# it takes a LONG time to run, so let's just try 2 hidden layers  \n",
    "# of 100 neurons each, using an ELU activation. We will also use \n",
    "# He initialization, which is generally recommended for ReLU \n",
    "# variants. \n",
    "\n",
    "# 5 points: Set up the neural net with 2 hidden layers of 100 \n",
    "#           neurons each.  Set parameter kernel_initializer\n",
    "#           to he_normal to override the default Glorot initization.\n",
    "#   Save your model into kerasELU\n",
    "# 1 point:  Call the summary method of kerasELU\n",
    "# 2 points: Compile kerasELU using the same settings as the\n",
    "#           previous keras model.\n",
    "# 2 points: Call the fit method of kerasELU on the training data,\n",
    "#           for 30 epochs, but using using the early stopping\n",
    "#           callback created above, and the validation data\n",
    "#           also created above for early stopping.\n",
    "#           BE CAREFUL TO USE THE CORRECT TRAINING DATA WE HAVE\n",
    "#           CREATED ABOVE FOR EARLY STOPPING AS DISCUSSED IN THE\n",
    "#           PREVIOUS CELL.\n",
    "#           Save the trained model into the variable kerasELU_History\n",
    "\n",
    "####################  insert your code below for 10 points ####################\n",
    "\n",
    "startTime = time.perf_counter() \n",
    "\n",
    "kerasELU = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # Compile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit\n",
    "kerasELU_History  = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### Your code ends above ##############################\n",
    "\n",
    "stopTime      = time.perf_counter()                                      \n",
    "print(f'\\nElapsed time: {stopTime - startTime:0.0f} seconds') \n",
    "print('Accuracy:', kerasELU.evaluate(X_test, y_test))\n",
    "\n",
    "playsound('yourcodeisdonerunning.m4a')  \n",
    "\n",
    "# When I used the same architecture as for sklearn above with 500, 1000, \n",
    "# and 2000 neurons in the hidden layers (4,059,510 parameters)\n",
    "# the accuracy was 0.5134 WITH early stopping (26 epochs), which is SLIGHTLY \n",
    "# lower than sklearn which I suspect uses Glorot init, not He.  \n",
    "# But it took 27 mins (1673 s). The model here with only 2 hidden layers \n",
    "# of 100 each (318,410 params) and He init, the time is reduced dramatically \n",
    "# to 130 seconds in 12 epochs, but the accuracy also dropped to 0.4763\n",
    "# vs. 0.4945 withOUT early stopping.  \n",
    "\n",
    "# Is the extra time worth the sacrifice in accuracy?  There is no universal\n",
    "# answer.  It depends on how important even a small gain in accuracy is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77602180-d848-4484-8e4d-3b40c9bb578e",
   "metadata": {},
   "source": [
    "#### Reducing Dimensionality with an Autoencoder\n",
    "\n",
    "In the previous assignment you learned how to reduce the dimensionality of your data by using principal component analysis (PCA), which can both speed up your training and improve accuracy (though not always!)  Here is another, fascinating way to reduce dimensionality.  I have written the code that does this reduction for you, but please continue reading so you'll know how to use it.  If you wish, you are welcome to run this code too, but if you do, be forwarned!  It took well over an hour to run on my computer.  \n",
    "\n",
    "An autoencoder is a network that learns how to reproduce its input.  That sounds trivial, but an autoencoder does it in an extremely interesting way that is certainly NOT trivial.  Here's a short summary, but if you want to learn more, you can find a detailed discussion in Géron chapter 17.\n",
    "\n",
    "An autoencoder (ae) has two neural networks.  The first is an encoder and the second is a decoder.  The encoder accepts the raw input data, in this case our CIFAR-10 data with 3,072 features, and represents it with a much shorter vectore of numbers.  We will use 512 numbers.  The decoder takes that new, reduced representation of the data, and tries to reconstruct the original input.  The decoder's output is compared to the encoder's input, and the amount of error becomes the loss function, which is used to modify the parameters of both the encoder and decoder using gradient descent.  \n",
    "\n",
    "The next 2 cells have the code for the autoencoder.  You should look at it and you have enough knowledge by now to understand it.  What you will do for your next task further below is to use only the encoder component to create new dimensionally reduced data to use for a new Keras classifier model to see if this method of dimensionality reduction has any benefit for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-crawford",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can run the code in this cell if you wish, \n",
    "# but it is not necessary.\n",
    "\n",
    "# Below, you will see that I commented out 2 lines, one in the \n",
    "# encoder and the other in the decoder in order to end up with a \n",
    "# reduced dimensionality of 512 features instead of 256.  \n",
    "\n",
    "# In this cell, we define both the encoder and the decoder.\n",
    "# Then, we combine both of them by concatenating them into a \n",
    "# single model called a 'stacked autoencoder' in the variable\n",
    "# stacked_ae  We finish be displaying that model's summary.\n",
    "# Notice how many parameters we have: almost 18 million, all\n",
    "# of which have to be learned by gradient descent!  This is \n",
    "# why the model takes much longer than what you have seen\n",
    "# previously in the training phase.\n",
    "\n",
    "tensorflow.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "stacked_encoder = keras.models.Sequential([ # Notice how each layer reduces the dimensionality\n",
    "    keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    keras.layers.Dense(2048, activation = \"selu\"), \n",
    "    keras.layers.Dense(1024, activation = \"selu\"),\n",
    "    keras.layers.Dense(512,  activation = \"selu\"),\n",
    "#    keras.layers.Dense(256,  activation = \"selu\"),\n",
    "])\n",
    "stacked_decoder = keras.models.Sequential([ # The decoder restores the original dimensionality\n",
    "#    keras.layers.Dense(512,         activation=\"selu\", input_shape=[256]),\n",
    "    keras.layers.Dense(1024,        activation=\"selu\", input_shape=[512]),\n",
    "    keras.layers.Dense(2048,        activation=\"selu\"),\n",
    "    keras.layers.Dense(32 * 32 * 3, activation=\"sigmoid\"), \n",
    "    keras.layers.Reshape([32, 32, 3])\n",
    "])\n",
    "stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder]) \n",
    "# stacked_ae is a concatenation of the 2 models just defined\n",
    "\n",
    "stacked_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-england",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#       YOU DO NOT NEED TO RUN THE CODE IN THIS CELL.       #\n",
    "# It is here only for you to read, and a few cells below,   #\n",
    "# you will find code to LOAD the result of this model so    #\n",
    "# you can use it without spending a couple of hours to wait #\n",
    "# for it to run on your computer.                           #\n",
    "#############################################################\n",
    "\n",
    "stacked_ae.compile(loss      =\"mse\",       # mse because we just need to compare \n",
    "                                           # the output vector with the input vector\n",
    "                   optimizer = \"Adam\", \n",
    "                   metrics   =['accuracy']) \n",
    "\n",
    "stacked_ae_history = stacked_ae.fit(X_train, X_train, \n",
    "                                    epochs         = 20, \n",
    "                                    validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611925d2-04f4-4604-b6f3-76d413ca6534",
   "metadata": {},
   "source": [
    "#### The 2 preceding cells are the autoencoder. \n",
    "The next cell shows you how to save a trained ML model.  By doing so, you can easily read it back in and then use it, as shown in the following cell.  I have already saved it, and given it to you for this assignment.  As long as you put that saved model into the same folder as this Jupyter notebook, then you will be able to succesfully read it back into memory in the cell that follows the save() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e05ef2-b1c7-423e-8433-4e02f867ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to run this code either.  It is here\n",
    "# just to show you how to save a model after training.\n",
    "\n",
    "# stacked_ae.save(\"cifar10_autoencoder.h5\")\n",
    "# stacked_encoder.save(\"cifar10EncoderModel.h5\")\n",
    "\n",
    "# The next cell shows you how to read a pre-saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a479747-1f52-4069-86c4-6e281cff4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DO NEED TO RUN THIS CODE!!\n",
    "\n",
    "# Read in a previously saved encoder model, and use it\n",
    "# to reduce the dimensionality of your training data\n",
    "\n",
    "# I have placed this saved model into Moodle for you so you \n",
    "# can run this cell\n",
    "\n",
    "stacked_encoder     = keras.models.load_model(\"cifar10EncoderModel.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83cc26-49c8-40c7-a959-d753101e956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use the encoder model to reduce some\n",
    "# CIFAR-10 image data into vectors of length 512 \n",
    "\n",
    "X_train_encoded = stacked_encoder.predict(X_train_) # first 45000 training images\n",
    "X_valid_encoded = stacked_encoder.predict(X_valid)  # last   5000 training images\n",
    "X_test_encoded  = stacked_encoder.predict(X_test)   # all 10000 test images\n",
    "\n",
    "print(X_train_encoded.shape)\n",
    "print(X_valid_encoded.shape)\n",
    "print(X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6fe50-f089-45be-8df0-7311fd237b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, Load the saved AUTOENCODER and use it to reconstruct the test images\n",
    "\n",
    "stacked_ae     = keras.models.load_model(\"cifar10_autoencoder.h5\") # load the ae from storage\n",
    "ae_test_images = stacked_ae.predict(X_test)                        # reproduce the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e988630-9043-42df-b48f-cf5b032beb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at one original image vs. its autoencoded version\n",
    "\n",
    "testImage      = 3 # Change this to see different pictures\n",
    "\n",
    "plt.imshow(X_test[testImage])         # original image\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(ae_test_images[testImage]) # reconstructed image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5432a6d5-4c81-412e-94f6-6d270787c0c4",
   "metadata": {},
   "source": [
    "As you can see, the reconstructed images are not very good, are they?  Surprisingly, they are actually significantly better if you include the 2 lines of the autoencoder that I commented out for training, even though those two lines cause the reduction to continue shrinking down to only 256 dimensions! Here is one image (before and after) from the training data that was reconstructed from just 256 numbers.  But let's move on for now.\n",
    "\n",
    "<img src='cifar10_Original.png' width=\"300\" height=\"300\"> <img src='cifar10_autoenc.png' width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c18df-7911-4ab3-88d9-d8497bce1eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 7: 10 points\n",
    "\n",
    "# Now we will construct a classifier with the same architecture \n",
    "# as in the ELU model we built above for Task 6, \n",
    "# but modified to accept the reduced dimensionality data.\n",
    "# Will it do better or worse than the previous model?\n",
    "\n",
    "tensorflow.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 3 points: Because the input data created from stacked_encoder above is\n",
    "#           already flat, you do not need to begin with a Flatten layer.\n",
    "#           Simply add a Dense layer of 100 neurons, using an 'elu' activation,\n",
    "#           and 'he_normal' for the weight initializations. This layer also \n",
    "#           serves as the input layer as long as you also add the input_shape\n",
    "#           parameter set to (512, )\n",
    "# 1 point:  Add another Dense layer just like the first one EXCEPT that you \n",
    "#           should not include the input_shape parameter this time.\n",
    "# 1 point:  Add your softmax output layer.\n",
    "\n",
    "# Save that model definition into the variable autoEncodedELU\n",
    "\n",
    "# 1 point:  Call the summary method of autoEncodedELU\n",
    "# 1 point:  Compile autoEncodedELU with a loss function of sparse\n",
    "#           categorical crossentropy, using the Adam optimizer, and\n",
    "#           accuracy as the only metric.\n",
    "# 3 points: Call the fit method of autoEncodedELU on the X_train_encoded\n",
    "#           data that was created above by using the encoder model, and\n",
    "#            y_train_  (Don't forget that trailing underscore at the end!),\n",
    "#            run it for 30 epochs using your \n",
    "#           early stopping callback that we created earlier, and also\n",
    "#           using the correct validation data, i.e. X_valid_encoded\n",
    "#           that was created above with the encoder and, of course, y_valid\n",
    "#           Save the trained model into autoEncodedELU_History\n",
    "\n",
    "####################  insert your code below for 10 points ####################\n",
    "\n",
    "startTime = time.perf_counter() \n",
    "\n",
    "autoEncodedELU = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit\n",
    "autoEncodedELU_History  = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### Your code ends above ##############################\n",
    "\n",
    "stopTime      = time.perf_counter()                                      \n",
    "print(f'\\nElapsed time: {stopTime - startTime:0.0f} seconds') \n",
    "print('autoEncodedELU Accuracy:', autoEncodedELU.evaluate(X_test_encoded, y_test))\n",
    "\n",
    "playsound('yourcodeisdonerunning.m4a')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3a891-ff21-4295-ad55-cd1ee5d5cf6c",
   "metadata": {},
   "source": [
    "Before ae encoding I obtained 0.4763 accuracy in 130 seconds on the full 3072-dimensional data. With the reduced 512-dimensional data I got 0.4621 in 110 seconds. So we gave up some accuracy for a marginal time savings, but considering the before/after images above, I think you'll agree that the small loss of accuracy is actually rather remarkable.  You might be curious enough now to investigate autoencoders more thoroughly on your own.\n",
    "\n",
    "#### What's next?\n",
    "In our quest to get higher accuracy we will build one more model in this assignment.  If you look at the Keras documentation for layer activations here:\n",
    "\n",
    "https://keras.io/api/layers/activations/ \n",
    "\n",
    "you will see at the bottom of that there are two activations referred to as 'advanced'.  One of them is LeakyReLU.  Even though it's advanced, it is still quite easy to use, but instead of setting the activation parameter of a Dense layer, you just omit the activation parameter, and instead add a LeakyReLU layer.  Yes, it's a LAYER, not a function.  There is a trivial example at the bottom of the URL I just gave you.  We'll use this in our last model for this assignment.\n",
    "\n",
    "In assignment 6 we saw that using PCA we were able to get our best model so far.  Since dimension reduction with autoencoders did not give us any major improvement, why not try PCA here with a neural net?  That's what we will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a8eab-fa19-4899-9ef4-795f9c757a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 8: 15 points\n",
    "\n",
    "# NN on PCA data\n",
    "\n",
    "startTime     = time.perf_counter()    \n",
    "\n",
    "numComponents = 100 # You can experiment with this after you turn in the assignment\n",
    "\n",
    "# First we will transform our data using PCA, then we'll build a new model\n",
    "# using the LeakyReLU activation layer.\n",
    "\n",
    "# 2 points: Create a PCA model with the number of components set to \n",
    "#           numComponents that was just defined.  This time use a\n",
    "#           random state of only 2, similar to what we did last week.\n",
    "#           Save this model into variable pcaModel\n",
    "# 1 point:  Call this fit method of pcaModel on X_train_flat as you did\n",
    "#           last week.\n",
    "# 1 point:  Now using that trained pcaModel, call its transform method\n",
    "#           on X_train_flat and save the result into X_train_PCA\n",
    "# 1 point:  Do the same for X_test_flat, saving it into X_test_PCA\n",
    "\n",
    "# If you did this correctly, then the 2 print statements will show you\n",
    "# that the shapes are (50000, 100) and (10000, 100)\n",
    "\n",
    "# Now you will build your model using the LeakyReLU layer:\n",
    "# 2 points: Build your model with a Dense layer of 200 neurons\n",
    "#           and set the input shape for this layer to (numComponents,)\n",
    "# 1 point:  Now add a LeakyReLU layer that uses the default settings, \n",
    "#           i.e. LeakyReLU()\n",
    "# 1 point:  Add another Dense layer of 200 neurons, nothing else.\n",
    "# 1 point:  Add another LeakyReLU layer which is the same as the first one\n",
    "# 1 point:  Add your Softmax output layer.\n",
    "# Save this model definition into pcaLeaky\n",
    "\n",
    "# 1 point:  Compile pcaLeaky with the same parameters as in Task 7\n",
    "# 1 point:  Call the summary method for pcaLeaky\n",
    "# 2 points: Fit pcaLeaky on the X_train_PCA and y_train data\n",
    "#           for 30 epochs, saving the fitted model into pcaLeakyHistory\n",
    "\n",
    "####################  insert your code below for 15 points ####################\n",
    "\n",
    "pcaModel  = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_PCA = \n",
    "\n",
    "X_test_PCA  = \n",
    "\n",
    "print('\\n X_train_PCA Shape:', X_train_PCA.shape)\n",
    "print('\\n X_test_PCA Shape:',  X_test_PCA.shape)\n",
    "\n",
    "pcaLeaky = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit\n",
    "pcaLeakyHistory = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### Your code ends above ##############################\n",
    "stopTime        = time.perf_counter()                                      \n",
    "print(f'\\nElapsed time: {stopTime - startTime:0.0f} seconds') \n",
    "print('Accuracy:', pcaLeaky.evaluate(X_test_PCA, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
