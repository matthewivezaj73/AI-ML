{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e66e9ce-b07c-4c1e-a94b-7add9345fbe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment 8: 75 points \n",
    "## Neural Networks: Advanced Techniques and Convolutional Neural Network\n",
    "\n",
    "## Special Notice: for this last assignment, you may take 2 weeks to work on it if you wish to see/read the material in Week 10 for the last task, but that is optional.  I have given you enough information to do the last task even before Week 10 readings and lectures. \n",
    "\n",
    "### IMPORTANT: \n",
    "#### You MUST read everything in tnis notebook CAREFULLY, including ALL code comments.  If you do not, then you may easily make mistakes.\n",
    "\n",
    "Be sure to review the class slides if you need to. (But read the comments in this notebook first.)\n",
    "\n",
    "Detailed documentation for Transfer Learning, written by the creator of Keras, is here:\n",
    "\n",
    "https://keras.io/guides/transfer_learning/ \n",
    "\n",
    "Keras API Topic Reference is here:\n",
    "\n",
    "https://keras.io/api/\n",
    "\n",
    "For the last task in this assignment, it is not necessary, but if you want to see all of the options for convolutional layers you can look at the documention here:\n",
    "\n",
    "https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "\n",
    "And for pooling layers (again not really necessary) you can look at:\n",
    "\n",
    "https://keras.io/api/layers/pooling_layers/\n",
    "\n",
    "### Heads up: Some of these models will take some time to run, so use playsound (or beepy) and make productive use of your time while you are waiting for them to finish training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5c7213e-36ce-4d60-9f49-60837ee167f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3222271599.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[44], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install --upgrade playsound\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Task 1: 5 points.  Set up environment\n",
    "\n",
    "####################################################################################\n",
    "# If some of these do not import properly, you may need to install them and re-run #\n",
    "####################################################################################\n",
    "!pip install playsound\n",
    "import keras\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import time\n",
    "    \n",
    "import matplotlib         as mpl   \n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy              as np   \n",
    "import pandas             as pd\n",
    "import beepy              as bp\n",
    "import playsound          as ps\n",
    "\n",
    "from keras.datasets          import cifar10  \n",
    "from pprint                  import pprint   \n",
    "\n",
    "from sklearn.cluster         import KMeans\n",
    "from sklearn.decomposition   import PCA\n",
    "from sklearn.ensemble        import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model    import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics         import confusion_matrix, precision_recall_curve, precision_score, recall_score, f1_score, silhouette_score, homogeneity_score, completeness_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\n",
    "from sklearn.neural_network  import MLPClassifier\n",
    "from sklearn.pipeline        import make_pipeline\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.svm             import LinearSVC, SVC\n",
    "from sklearn.tree            import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from yellowbrick.classifier  import ClassBalance, ClassificationReport, ClassPredictionError, ConfusionMatrix\n",
    "from yellowbrick.cluster     import SilhouetteVisualizer\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "# Make this notebook's output stable across runs\n",
    "tensorflow.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ace49577-8e8d-4a0f-96da-5c55a3939ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prep the CIFAR-10 data\n",
    "\n",
    "# Note that I have added new lines here \n",
    "# that relate to the use of early stopping\n",
    "# AND I have deleted the lines that created the \n",
    "# input data which we do not need for these neural networks\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "\n",
    "X_train  = X_train.astype('float32')\n",
    "X_test   = X_test.astype('float32')\n",
    "\n",
    "# Normalize the data\n",
    "X_train /= 255.0  # The largest number is 255, and the smallest 0\n",
    "X_test  /= 255.0  # So this division will normalize the data.\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:] # 1st 5000 for validation in early stopping #\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:] # X_train and y_train for early stopping    #\n",
    "\n",
    "# We also have to use ravel to change the target values (the values we want to predict). \n",
    "y_train = np.ravel(y_train)\n",
    "y_valid = np.ravel(y_valid)\n",
    "y_test  = np.ravel(y_test)\n",
    "\n",
    "LABEL_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "'Done' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885561c6-08a2-468f-af44-89f726b83672",
   "metadata": {},
   "source": [
    "In my chapter 11 lecture slide called 'Network Architecture using Batch Norm' you can see some Keras code for a small network.  For this model we will build our first serious, deep model which will also use Batch Normalization (BN).  There are numerous, significant benefits of BN, so it's important to know how to use it.  One of those benefits is that you can often use a larger learning rate, which helps to significantly reduced training time.  Let's try that in this next model.  \n",
    "\n",
    "BN is easily implemented, simply by adding the BN layer before each Dense layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8478fc0c-685c-4237-a2ef-eb7e3a44e8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (Flatten)       (None, 3072)              0         \n",
      "                                                                 \n",
      " hidden_layer_1 (Dense)      (None, 300)               921900    \n",
      "                                                                 \n",
      " batch_norm_1 (BatchNormaliz  (None, 300)              1200      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " hidden_layer_2 (Dense)      (None, 200)               60200     \n",
      "                                                                 \n",
      " batch_norm_2 (BatchNormaliz  (None, 200)              800       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " hidden_layer_3 (Dense)      (None, 100)               20100     \n",
      "                                                                 \n",
      " batch_norm_3 (BatchNormaliz  (None, 100)              400       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " hidden_layer_4 (Dense)      (None, 90)                9090      \n",
      "                                                                 \n",
      " batch_norm_4 (BatchNormaliz  (None, 90)               360       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " hidden_layer_5 (Dense)      (None, 80)                7280      \n",
      "                                                                 \n",
      " batch_norm_5 (BatchNormaliz  (None, 80)               320       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " hidden_layer_6 (Dense)      (None, 70)                5670      \n",
      "                                                                 \n",
      " batch_norm_6 (BatchNormaliz  (None, 70)               280       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " hidden_layer_7 (Dense)      (None, 60)                4260      \n",
      "                                                                 \n",
      " batch_norm_7 (BatchNormaliz  (None, 60)               240       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                610       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,032,710\n",
      "Trainable params: 1,030,910\n",
      "Non-trainable params: 1,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 1.8459 - accuracy: 0.3360 - val_loss: 2.0716 - val_accuracy: 0.2754\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.7140 - accuracy: 0.3861 - val_loss: 1.7964 - val_accuracy: 0.3544\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.6486 - accuracy: 0.4102 - val_loss: 1.9024 - val_accuracy: 0.3369\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5939 - accuracy: 0.4308 - val_loss: 1.6363 - val_accuracy: 0.4210\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5551 - accuracy: 0.4464 - val_loss: 1.5802 - val_accuracy: 0.4374\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.5207 - accuracy: 0.4585 - val_loss: 1.5614 - val_accuracy: 0.4444\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4848 - accuracy: 0.4721 - val_loss: 1.6675 - val_accuracy: 0.4162\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4579 - accuracy: 0.4816 - val_loss: 1.4919 - val_accuracy: 0.4699\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4378 - accuracy: 0.4901 - val_loss: 1.4786 - val_accuracy: 0.4730\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4157 - accuracy: 0.4965 - val_loss: 1.4923 - val_accuracy: 0.4679\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.4001 - accuracy: 0.5021 - val_loss: 1.5818 - val_accuracy: 0.4564\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3777 - accuracy: 0.5119 - val_loss: 1.5036 - val_accuracy: 0.4615\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.4786 - accuracy: 0.4730\n",
      "Test Accuracy: 47.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 277 for command:\n",
      "        open \"C:\\Users\\matth\\OneDrive\\Desktop\\yourcodeisdonerunning.m4a\"\n",
      "    A problem occurred in initializing MCI.\n",
      "\n",
      "    Error 305 for command:\n",
      "        close \"C:\\Users\\matth\\OneDrive\\Desktop\\yourcodeisdonerunning.m4a\"\n",
      "    Cannot specify extra characters after a string enclosed in quotation marks.\n",
      "Failed to close the file: \"C:\\Users\\matth\\OneDrive\\Desktop\\yourcodeisdonerunning.m4a\"\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "\n    Error 277 for command:\n        open \"C:\\Users\\matth\\OneDrive\\Desktop\\yourcodeisdonerunning.m4a\"\n    A problem occurred in initializing MCI.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPlaysoundException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 113\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Play a sound to indicate completion\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m \u001b[43mps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaysound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmatth\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43myourcodeisdonerunning.m4a\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\playsound.py:72\u001b[0m, in \u001b[0;36m_playsoundWin\u001b[1;34m(sound, block)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mwinCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopen \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     winCommand(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplay \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sound, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m wait\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     74\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda\\lib\\site-packages\\playsound.py:64\u001b[0m, in \u001b[0;36m_playsoundWin.<locals>.winCommand\u001b[1;34m(*command)\u001b[0m\n\u001b[0;32m     60\u001b[0m     exceptionMessage \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Error \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(errorCode) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for command:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m command\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     62\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m errorBuffer\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     63\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(exceptionMessage)\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PlaysoundException(exceptionMessage)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mPlaysoundException\u001b[0m: \n    Error 277 for command:\n        open \"C:\\Users\\matth\\OneDrive\\Desktop\\yourcodeisdonerunning.m4a\"\n    A problem occurred in initializing MCI."
     ]
    }
   ],
   "source": [
    "# Task 2: 20 points\n",
    "\n",
    "# Here we will build a BN neural net with 7 hidden layers with 300, 200, 100, 90, \n",
    "# 80, 70, and 60 neurons.  The math works out to be 1,044,998 neurons with \n",
    "# 7,944 of them not trainable.  Those are the parameters associated with the BN equations.\n",
    "# With over a million trainable parameters you might expect this to run quite slowly,\n",
    "# and if you just use the default hyperparameters it will run slowly.  However,\n",
    "# one of the many benefits of BN is that you can often use a larger learning rate,\n",
    "# which can dramatically reduce training time.  The default learning rate is 0.001\n",
    "# but we will use a rate that is 5 times larger than that.\n",
    "\n",
    "# We will also use early stopping to take advantage of the larger learning rate.\n",
    "# This is an easy way to modify the default learning rate:\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.005)    # default 0.001\n",
    "\n",
    "# 5 points: Create a sequential model with 7 hidden layers of 300, 200, 100, 90, 80\n",
    "#           70, and 60 neurons.  All of them should use the 'elu' activation function\n",
    "#           and 'he_normal' kernel initializer.  You can refer to assignment 7\n",
    "#           if you forget how to set those parameters.\n",
    "#           Of course you will have a Flatten layer in the beginning that accepts\n",
    "#           an input shape of [32, 32, 3] and a Dense output layer using 'softmax'\n",
    "#           activation for its 10 neurons.\n",
    "# 2 points: Add a BatchNormalization layer BEFORE each of the hidden layers, \n",
    "#           as well as a BatchNormalization layer in front of the output layer. \n",
    "#           Don't change any of the BN parameter default settings. \n",
    "# 2 points: In Keras you can give your own customized name to each layer\n",
    "#           by setting the parameter 'name' to a string variable,\n",
    "#           for example: name = 'My_Layer_1', or anything else as long is\n",
    "#           it does not use any spaces. So add a name of your choice to each \n",
    "#           of your layers. This helps when looking at the summary.\n",
    "\n",
    "#   Save the above architecture as the value of the variable cifar10_BN\n",
    "\n",
    "# 1 point:  Call the summary method of cifar10_BN and verify that you see\n",
    "#           1,037,054 trainable parameters and 7,944 non-trainable.\n",
    "# 3 points: Compile this model using sparse_categorical_crossentropy loss,\n",
    "#           the 'accuracy' metric, and set optimizer = opt, which I \n",
    "#           already defined above for you.\n",
    "# 2 points: Define your early stopping callback as you did in the previous\n",
    "#           assignment with restore_best_weights set to True, but set patience to 3\n",
    "#           Save that callback into the variable early_stopping_cb\n",
    "# 4 points: Call the fit method on cifar10_BN.  Because we are using early\n",
    "#           stopping you will need to pass it X_train and y_train and use\n",
    "#           both X_valid and y_valid for your validation data to trigger\n",
    "#           early stopping. Don't forget to add the reference to early_stopping_cb\n",
    "#           inside your call to the fit method. Consult Assignment 7 if you need to.\n",
    "#           Save the results into variable cifar10_BN_History\n",
    "# 1 point:  Before and after your fit method, add the necessary code\n",
    "#           to capture the start and stop time, print out the elapsed time\n",
    "#           print out the accuracy score on the test data, and call playsound.\n",
    "\n",
    "####################  insert your code below  ####################\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.005)                                                                     # Defining the learning rate for the optimizer.\n",
    "\n",
    "\n",
    "model = keras.Sequential()                                                                                           # Creating a blank sequential model.\n",
    "\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3), name='Input_Layer'))                                         # Adding the input layer.\n",
    "\n",
    "\n",
    "layer_sizes = [300, 200, 100, 90, 80, 70, 60]                                                                        # Add BatchNormalization and hidden layers.\n",
    "for i, size in enumerate(layer_sizes):\n",
    "    model.add(keras.layers.BatchNormalization(name=f'BatchNorm_{i+1}'))\n",
    "    model.add(keras.layers.Dense(size, activation='elu', kernel_initializer='he_normal', name=f'Hidden_Layer_{i+1}'))# Making a for loop to add each hidden layer.\n",
    "\n",
    "\n",
    "model.add(keras.layers.BatchNormalization(name='BatchNorm_Output'))                                                  # Adding the Output layer.\n",
    "model.add(keras.layers.Dense(10, activation='softmax', name='Output_Layer'))                                         # Adding output layer with softmax activation.\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.005)\n",
    "cifar10_BN.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])                      # Compile the model.\n",
    "\n",
    "model.summary()                                                                                                      # Display model summary.\n",
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)                             # Define early stopping callback.\n",
    "\n",
    "\n",
    "start_time = time.time()                                                                                             # Train the model with early stopping.\n",
    "\n",
    "cifar10_BN_History = cifar10_BN.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  \n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping_cb]\n",
    ")\n",
    "\n",
    "end_time = time.time()                                                                                               # Grabbing the end time.\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "test_loss, test_accuracy = cifar10_BN.evaluate(X_test, y_test)                                                       # Evaluate the model on test data\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")                                                                         # Printing out the test accuracy.\n",
    "\n",
    "\n",
    "playsound(\"/Users/matthewivezaj/Desktop/yourcodeisdonerunning.m4a\")                                                  # Play a sound to indicate completion\n",
    "\n",
    "# Don't forget to add your code for timing and output messages\n",
    "####################  insert your code above  ####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf377b3-44cf-4df7-b7c4-c7c10c2db0a1",
   "metadata": {},
   "source": [
    "For me that only ran for 11 epochs (about 10 minutes) and had accuracy of 0.5231999754905701, only slightly better than my best previous model, though your results may be somewhat different.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97358f8b-dc75-4b3c-9eba-59dc9e83ab25",
   "metadata": {},
   "source": [
    "#### Transfer Learning\n",
    "We have discussed transfer learning before and I also discuss it again in my Chapter 11 lecture.  The idea is to load a pretrained model that operates on data that is similar to yours, and use it to recognize the low-level features of your data.  But you can delete the top layer(s) from the pretrained model, and replace it with your own top layer(s) in order to fine tune it for your specific data.  For our next model, we will use a famous predefined model called ResNet-50, which you can download directly from the web.  Let's see how it does on the CIFAR-10 data, even though it was not trained on CIFAR-10.\n",
    "\n",
    "ResNet-50 has 107 layers and was trained on ImageNet, which has (the last time I looked at it) more than 14 million high-resolution, color images in 1,000 categories. So the fact that it, and many other large models, are freely available to anyone to download and use from the web is a great service to the machine learning community.  Most individuals simply do not have enough computing power at home to train such models.\n",
    "\n",
    "You can learn more about ImageNet at: https://www.image-net.org/about.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "def6d474-b0e3-4660-86ca-e63d0a7c8340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 35s 0us/step\n",
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 38, 38, 3)    0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 16, 16, 64)   9472        ['conv1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 16, 16, 64)   256         ['conv1_conv[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 16, 16, 64)   0           ['conv1_bn[0][0]']               \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 18, 18, 64)   0           ['conv1_relu[0][0]']             \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 8, 8, 64)     0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 8, 8, 64)     4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 8, 8, 256)    16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 8, 8, 256)    0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 8, 8, 64)     16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 8, 8, 64)     36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 8, 8, 64)    256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 8, 8, 64)    0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 8, 8, 256)    16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 8, 8, 256)    0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 8, 8, 256)    0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 4, 4, 128)    32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 4, 4, 512)    131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 4, 4, 512)    0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 4, 4, 512)    0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 4, 4, 128)    65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 4, 4, 128)    147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 4, 4, 128)   512         ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 4, 4, 128)   0           ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 4, 4, 512)    66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 4, 4, 512)   2048        ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 4, 4, 512)    0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 4, 4, 512)    0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 2, 2, 256)    131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 2, 2, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                                                  'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block1_out[0][0]',       \n",
      "                                                                  'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block2_out[0][0]',       \n",
      "                                                                  'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block3_out[0][0]',       \n",
      "                                                                  'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block4_out[0][0]',       \n",
      "                                                                  'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block5_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 2, 2, 256)    262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 2, 2, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 2, 2, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 2, 2, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 2, 2, 1024)   263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 2, 2, 1024)  4096        ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 2, 2, 1024)   0           ['conv4_block5_out[0][0]',       \n",
      "                                                                  'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 2, 2, 1024)   0           ['conv4_block6_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 1, 1, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 1, 1, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 1, 1, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 1, 1, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 1, 1, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 1, 1, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 1, 1, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 1, 1, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 1, 1, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 1, 1, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 1, 1, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " max_pool (GlobalMaxPooling2D)  (None, 2048)         0           ['conv5_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet-50\n",
    "\n",
    "# This cell will download ResNet-50 from GitHub in a minute or less.\n",
    "# The parameter settings are telling it not to include the top layer\n",
    "# because ResNet was trained to recognize 1,000 categories of images in\n",
    "# ImageNet.  But we are INCLUDING the pretrained imagenet weights because \n",
    "# they will be used to recognize our low-level image features in CIFAR-10.\n",
    "# We have to tell it that, for our data, the input shape is (32, 32, 3)\n",
    "# or else it will assume input shapes of (224, 224, 3).\n",
    "# Finally, we are telling it to use max pooling, which is a concept\n",
    "# I will discuss in my Chapter 14 lecture on Convolutional Neural Nets.\n",
    "\n",
    "resnet50_model = tensorflow.keras.applications.ResNet50(include_top=False,       weights=\"imagenet\", \n",
    "                                                        input_shape=(32, 32, 3), pooling='max') # Max pooling flattens the last layer from 1,1,2048 to 2048\n",
    "                                                                                              # and gives better results than avg pooling\n",
    "resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2e5fd-08e3-475a-add3-8097ba612050",
   "metadata": {},
   "source": [
    "Notice how many parameters there are -- nearly 24 MILLION!  Also notice that 53,120 are not trainable because they are parameters related to the Batch Normalization layers which you can see in the summary output.  Take a few moments to scan through that summary and look at the different types of layers.  \n",
    "\n",
    "Since this model is already trained, we will only use it to convert our CIFAR-10 images into a flat vector of 2048 numbers.  Notice that the final layer in the summary above shows that the output (after removing the top layer) has shape (1, 1, 2048), i.e. 1 row with only 1 column and a third dimension of 2048 features. \n",
    "\n",
    "So instead of using the 3072 features of CIFAR-10, we will use those 2048 in a similar manner to how we previously used a dimensionally-reduced set of features created by an autoencoder or by using principal component analysis.  But we would expect the ResNet features to give us better accuracy since they encode the low-level feature knowledge that ResNet-50 learned when it was trained on the massive ImageNet dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d3d11d4-4fb7-41ba-a456-842cf6251eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 68s 43ms/step\n",
      "313/313 [==============================] - 14s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ATTENTION: This cell will take time to run, almost 7 minutes\n",
    "# on my computer, so just let it run until finished.  It is slow\n",
    "# due to a required preprocessing step described next.\n",
    "\n",
    "# We need to re-create our training and test data because \n",
    "# ResNet-50 requires us to preprocess our data with a custom\n",
    "# algorithm that does 2 things:\n",
    "#     First, it changes the order of the 3 color channels.\n",
    "#     CIFAR-10 uses RGB, but ResNet-50 wants to see BGR.\n",
    "#     Second, the preprocessor will zero-center the data \n",
    "#     but does not scale it.  So you will not see our previous\n",
    "#     code that normalized the data by dividing it by 255.\n",
    "\n",
    "# You can see this preprocessing function below called 'preprocess_input'.\n",
    "# There are several other large models, all available from the Keras\n",
    "# website, that have their own custom preprocessors.\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "\n",
    "X_train  = X_train.astype('float32')\n",
    "X_test   = X_test.astype('float32')\n",
    "\n",
    "# Process the training and test data, as required by ResNet-50, and then\n",
    "# use the predict method to convert our 3072 features down to 2048.\n",
    "X_train = resnet50_model.predict(keras.applications.resnet.preprocess_input(X_train))\n",
    "X_test  = resnet50_model.predict(keras.applications.resnet.preprocess_input(X_test))\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:] # 1st 5000 for validation in early stopping \n",
    "y_valid, y_train = y_train[:5000], y_train[5000:] # X_train and y_train modified for early stopping  \n",
    "\n",
    "# We again use ravel to change the target values \n",
    "y_train = np.ravel(y_train)\n",
    "y_valid = np.ravel(y_valid)\n",
    "y_test  = np.ravel(y_test)\n",
    "\n",
    "LABEL_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "'Done' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6219241-fa88-41fe-ab34-d8f379773e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:\t (45000, 2048)\n",
      "X_valid shape:\t (5000, 2048)\n",
      "X_test shape:\t (10000, 2048)\n",
      "\n",
      "y_train shape:\t (45000,)\n",
      "y_valid shape:\t (5000,)\n",
      "y_test shape:\t (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the new shapes of our data:\n",
    "\n",
    "print('X_train shape:\\t', X_train.shape)\n",
    "print('X_valid shape:\\t', X_valid.shape)\n",
    "print('X_test shape:\\t',  X_test.shape)\n",
    "\n",
    "print('\\ny_train shape:\\t', y_train.shape)\n",
    "print('y_valid shape:\\t', y_valid.shape)\n",
    "print('y_test shape:\\t',  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c8bc7e8-3329-4125-a658-9fea82ef06b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 100)               204900    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,010\n",
      "Trainable params: 216,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 1.2183 - accuracy: 0.5847 - val_loss: 1.0333 - val_accuracy: 0.6462\n",
      "Epoch 2/30\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.9459 - accuracy: 0.6660 - val_loss: 0.9805 - val_accuracy: 0.6560\n",
      "Epoch 3/30\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.8458 - accuracy: 0.7000 - val_loss: 0.9712 - val_accuracy: 0.6730\n",
      "Epoch 4/30\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.7656 - accuracy: 0.7264 - val_loss: 1.0066 - val_accuracy: 0.6528\n",
      "Epoch 5/30\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.6852 - accuracy: 0.7552 - val_loss: 1.0344 - val_accuracy: 0.6606\n",
      "Epoch 6/30\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.6163 - accuracy: 0.7784 - val_loss: 1.0766 - val_accuracy: 0.6534\n",
      "Epoch 7/30\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.5427 - accuracy: 0.8051 - val_loss: 1.1550 - val_accuracy: 0.6624\n",
      "Epoch 8/30\n",
      "1407/1407 [==============================] - 2s 2ms/step - loss: 0.4798 - accuracy: 0.8285 - val_loss: 1.2444 - val_accuracy: 0.6566\n",
      "\n",
      "Elapsed time: 18 seconds\n",
      "313/313 [==============================] - 0s 846us/step - loss: 0.9924 - accuracy: 0.6549\n",
      "Accuracy: [0.992353618144989, 0.6549000144004822]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'playsound' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mElapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstopTime \u001b[38;5;241m-\u001b[39m startTime\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, transferTop\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test))\n\u001b[1;32m---> 63\u001b[0m \u001b[43mplaysound\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myourcodeisdonerunning.m4a\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'playsound' is not defined"
     ]
    }
   ],
   "source": [
    "# Task 3: 10 points\n",
    "\n",
    "# NEW TOP LAYER FOR THE TRANSFER MODEL\n",
    "\n",
    "# 5 points: Set up the top layer with 2 Dense layers of 100 \n",
    "#           neurons each and 'elu' activation.  Set parameter \n",
    "#           kernel_initializer to he_normal, and of course, you \n",
    "#           must add your Dense softmax output layer.  Make your \n",
    "#           first Dense layer the input layer by setting input_shape \n",
    "#           to [2048]. You have already done something similar \n",
    "#           in a previous assignment. Save the model into transferTop\n",
    "# 1 point:  Call the summary method of transferTop\n",
    "# 2 points: Compile transferTop with the sparse_categorical_crossentropy\n",
    "#           loss function, the Adam optimizer, and accuracy for metrics.\n",
    "# 2 points: Call the fit method of transferTop on the training data,\n",
    "#           for 30 epochs, but define an early stopping callback\n",
    "#           with patience=5 and use it with the validation data.  \n",
    "#           Save your fitted model result into transferTop_History\n",
    "#           You have also done something very similar before.\n",
    "\n",
    "####################  insert your code below  ####################\n",
    "\n",
    "startTime = time.perf_counter() \n",
    "\n",
    "# Definition of model\n",
    "transferTop = keras.Sequential([                                                                        # Creating a sequential model with 2 dense layers and adding in a softmax activation.\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', input_shape=(2048,)),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dense(10, activation='softmax')  \n",
    "])\n",
    "\n",
    "\n",
    "# Summary\n",
    "transferTop.summary()                                                                                   # Bringing up a summary of the model\n",
    "\n",
    "\n",
    "\n",
    "# Compile\n",
    "\n",
    "transferTop.compile(optimizer='adam',                                                                   # Compiling the model.\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "\n",
    "early_stopping_cb = EarlyStopping(patience=5, restore_best_weights=True)                                # Defining the early stopping callback and assigning it to a variable.\n",
    "\n",
    "\n",
    "# Fit\n",
    "transferTop_History  = transferTop.fit(X_train, y_train,                                        # Fitting the model on the training data and passing some other params.\n",
    "                                      epochs=30,\n",
    "                                      validation_data=(X_valid, y_valid),  \n",
    "                                      callbacks=[early_stopping_cb])\n",
    "\n",
    "\n",
    "########################### Your code ends above ##############################\n",
    "\n",
    "\n",
    "stopTime      = time.perf_counter()                                      \n",
    "print(f'\\nElapsed time: {stopTime - startTime:0.0f} seconds') \n",
    "print('Accuracy:', transferTop.evaluate(X_test, y_test))\n",
    "\n",
    "playsound('yourcodeisdonerunning.m4a') \n",
    "\n",
    "# In assg 7, this took 289s with all 3072 features, giving 0.49950000643730164\n",
    "# Now with 2048 features from Resnet50, it took 78 sec in only 9 iters for 0.6582000255584717, massively best model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb0d3a-8a68-4737-a0b3-48a5b8d57827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we must (yet again) reload and prepare our CIFAR-10\n",
    "# data because we are done using ResNet-50\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data() \n",
    "\n",
    "X_train  = X_train.astype('float32')\n",
    "X_test   = X_test.astype('float32')\n",
    "\n",
    "# Normalize the data\n",
    "X_train /= 255.0  # The largest number is 255, and the smallest 0\n",
    "X_test  /= 255.0  # So this division will normalize the data.\n",
    "\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:] # 1st 5000 for validation in early stopping #\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:] # X_train and y_train for early stopping  #\n",
    "\n",
    "# Again use ravel to change the target values\n",
    "y_train = np.ravel(y_train)\n",
    "y_valid = np.ravel(y_valid)\n",
    "y_test  = np.ravel(y_test)\n",
    "\n",
    "LABEL_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "'Done' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37bbbb-aa25-4f38-9b0f-08471fda839d",
   "metadata": {},
   "source": [
    "The remaining tasks use an architecture called Convolutional Neural Networks (CNN) that was specifically designed for images.  The original images were the MNIST handwritten digits, but CNNs have evolved for color images and remain the dominant architecture for images.  This assignment is being given to you in Week 9, and we aren't discussing CNNs in class until next week.  So you will learn about them then, but that does not prevent us from actually building some now.  This will help generate your curiosity for next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5a4ed-c66b-4344-98fa-91144a85be9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 4: 10 points\n",
    "\n",
    "# Baseline CNN Model\n",
    "\n",
    "# Note: This baseline model will not give very good results!\n",
    "\n",
    "# I have given you the start of some baseline code\n",
    "# for a CNN.  The Conv2D layers refer to the actual convolutions\n",
    "# you'll learn about next week.  These involve the creation of\n",
    "# filters, which are like small windows that will slide across\n",
    "# an image looking for imporant features, like lines, arcs, edges, etc.\n",
    "# Where you see filters=32 that means there are 32 such filters\n",
    "# that will learn to detect these low level features.  The kernel_size\n",
    "# parameter defines the size of these filters, in this case 3 pixels by 3 pixels.\n",
    "# You'll learn about padding next week. Convolutions are generally followed\n",
    "# by a pooling layer, usually max pooling as you see here, or average pooling.\n",
    "# Again you'll learn about these next week.  pool_size is also a\n",
    "# measurement in pixels. \n",
    "\n",
    "# 6 points:  Copy the 3 layers here (2 convolutional layers followed\n",
    "#            by a max pooling layer) and add 2 additional sets after\n",
    "#            them for a total of 9 layers - 3 sets of 3 layers.\n",
    "#            Remember that only the first convolutional layer accepts\n",
    "#            the (32, 32, 3) input images, so remove that from all\n",
    "#            the other convolutional layers.  You must also make one\n",
    "#            additional change to the second and third pairs of convolution\n",
    "#            layers.  In the second pair of convolutions, change the number\n",
    "#            of filters from 32 to 64.  And in the third pair of convolutions\n",
    "#            set the number of filters to 128.  After all this you will have\n",
    "#            3 sets of 2 convolutions followed by a max pooling layer,\n",
    "#            where set 1 has 32 filters in each convolution, the second set\n",
    "#            has 64 filters each, and the third set has 128 filters each.\n",
    "# 1 point:   After the 3rd max pooling layer, add a Flatten layer\n",
    "#            with no arguments.  This will take the output of that last\n",
    "#            max pooling layer and flatten it into a long vector, feeding\n",
    "#            it to the next layer.\n",
    "# 2 points:  After the Flatten layer, you'll add two Dense layers of 100\n",
    "#            neurons each, using the same activation function and\n",
    "#            kernel initializers that you see in the convolutional layers.\n",
    "# 1 point:   Finally, add your Dense softmax output layer with 10 neurons.\n",
    "\n",
    "# When you look at the summary, you should see that it has a total\n",
    "# of 13 layers.\n",
    "\n",
    "####################  insert your code below  ####################\n",
    "\n",
    "model = keras.Sequential()                                                                                                                                # Creating the blank sequential model.\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', input_shape=(32, 32, 3), name='Conv_Layer_1'))# Adding convolutional layer with 32 filters. \n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name='Conv_Layer_2'))                         # Adding convolutional layer with 32 filters. \n",
    "model.add(keras.layers.MaxPooling2D((2, 2), name='MaxPool_Layer_1'))                                                                                      # Adding maxpooling layer with input shape 2\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name='Conv_Layer_3'))                         # Adding convolutional layer with 64 filters. \n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name='Conv_Layer_4'))                         # Adding convolutional layer with 64 filters. \n",
    "model.add(keras.layers.MaxPooling2D((2, 2), name='MaxPool_Layer_2'))                                                                                      # Adding max pooling layer to match layers with 64 filters.      \n",
    "\n",
    "model.add(keras.layers.Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name='Conv_Layer_5'))                        # Adding convolutional layer with 128 filters. \n",
    "model.add(keras.layers.Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name='Conv_Layer_6'))                        # Adding convolutional layer with 128 filters. \n",
    "model.add(keras.layers.MaxPooling2D((2, 2), name='MaxPool_Layer_3'))                                                                                      # Adding max pooling layer to match layers with 128 filters. \n",
    "\n",
    "model.add(keras.layers.Flatten(name='Flatten_Layer'))                                                                                                     # Adding flattened layer.\n",
    "\n",
    "model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', name='Dense_Layer_1'))                                                # Adding dense layer with 100 neurons.\n",
    "model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', name='Dense_Layer_2'))                                                # Adding dense layer with 100 neurons.\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax', name='Output_Layer'))                                                                              # Output layer with 10 neurons for classification.\n",
    "\n",
    "model.summary()                                                                                                                                           # Displaying the summary.\n",
    "####################  insert your code above  ####################\n",
    "\n",
    "baseCNN.summary()\n",
    "\n",
    "opt               = keras.optimizers.Adam(learning_rate=0.001) # This is the default learning rate for Adam\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience            =3, \n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "baseCNN.compile(loss       = \"sparse_categorical_crossentropy\",\n",
    "                optimizer  = opt,\n",
    "                metrics    =[\"accuracy\"])\n",
    "# Fit\n",
    "startTime = time.perf_counter() \n",
    "\n",
    "baseCNN_History  = baseCNN.fit(X_train, y_train, \n",
    "                               epochs          = 20,         \n",
    "                               validation_data = (X_valid, y_valid),\n",
    "                               callbacks       = [early_stopping_cb]) \n",
    "\n",
    "stopTime      = time.perf_counter()                                      \n",
    "print(f'\\nElapsed time: {stopTime - startTime:0.0f} seconds') \n",
    "print('Accuracy:', baseCNN.evaluate(X_test, y_test))\n",
    "\n",
    "playsound('yourcodeisdonerunning.m4a') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a454b0-bf63-45dc-b4b8-d1a00d97a9fe",
   "metadata": {},
   "source": [
    "Like I said above, this model does not give very good results for an architecture that is specifically designed for images, right?  On my laptop it only had accuracy of 0.39430001378059387 and stopped in 11 minutes after only 4 epochs.  But the next and final task will be to MODIFY this model which will result in your best model BY FAR.  For that final model, you must write ALL of the code yourself. Of course, MUCH of it you have already done for other assignments and tasks, so you could even copy a portion from other cells and modify it for various parameter settings and variable names, etc.  But be careful not to make simple errors!\n",
    "\n",
    "When you think it's ready to run, double-check that you have:\n",
    "\n",
    "* defined your model\n",
    "* saved it into the variable revisedCNN\n",
    "* defined your (optional, but strongly advised) early stopping callback (I strongly suggest a patience of 3 epochs)\n",
    "* compiled your model with the loss function, a choice of optimizer, and accuracy for the evaluation metric\n",
    "* called the fit method on the training data\n",
    "* specified the maximum number of epochs\n",
    "* added a reference to the validation data for your callback (if you decide to use it)\n",
    "* added a reference to your early stopping callback (if you decide to use it)\n",
    "\n",
    "### Also make sure you do the REQUIRED step of retraining your final, preferred model for at least 10 epochs as described below.\n",
    "\n",
    "It's totally up to you if you want to add the timing and playsound alert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b93d79-d285-4838-962c-c76c758f151c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 5: 30 points \n",
    "\n",
    "# Revised CNN Model\n",
    "\n",
    "# Your final task is to REVISE the model just used and see how much better\n",
    "# you can make it with some combination of the following techniques.  By now,\n",
    "# you have already either used these techniques, or you have at least\n",
    "# seen them in the textbook.  \n",
    "\n",
    "# Here are some things to experiment with:\n",
    "# 1. Learning Rate\n",
    "# 2. Adding BatchNormalization layers, which you have already done above for Task 2\n",
    "#    (I strongly suggest you do this one to get better accuracy.)\n",
    "# 3. Adding Dropout layers.  You won't see any in the ResNet-50 summary, but\n",
    "#    Dropout is a common regularization technique to reduce overfitting.  You can \n",
    "#    find code examples in the textbook or in Gron's notebook for chapter 11. You \n",
    "#    can experiment with different values for the dropout rate. See Gron for advice.\n",
    "# 4. Adding additional Conv2D layers, followed by a pooling layer.\n",
    "# 5. You can try an AveragePooling2D layer instead of MaxPooling2D. For this,\n",
    "#    you can consult: https://keras.io/api/layers/pooling_layers/average_pooling2d/ \n",
    "# 6. For your Conv2D layers, you can try changing the kernel_size \n",
    "#    to (5, 5) instead of (3, 3)\n",
    "# 7. Anything else you can think of.\n",
    "\n",
    "# You do NOT have to try all of these techniques.  They are only ideas.\n",
    "\n",
    "# STRONGLY SUGGESTED:  If you add several of these changes all at once,\n",
    "#                      then if the model accuracy is either better or worse\n",
    "#                      you will not know which technique made it better/worse.\n",
    "#                      Thus, try adding one at a time, train the model and \n",
    "#                      check the accuracy results.  Some things may not give\n",
    "#                      any benefit at all or may make it MUCH better.\n",
    "#                      Because you will therefore be doing numerous training runs,\n",
    "#                      to avoid waiting huge amounts of time, try to run\n",
    "#                      the baseline model for only 2 epochs, and each revised one\n",
    "#                      for only 2 epochs.  Then evaluate the results and continue.\n",
    "\n",
    "# REQUIRED:            For your FINAL model after you decide which changes to keep,\n",
    "#                      train it for at least 10 epochs (more, if you prefer)\n",
    "# BE AWARE that this final model will likely take a LONG time to train.  Mine took\n",
    "#    2 hours, so I STRONGLY advise you to use early stopping so you don't have\n",
    "#    to wait any longer than necessary.\n",
    "\n",
    "# Without much effort at all, I was easily able to get my own modified base model\n",
    "# to achieve more than 78% accuracy, so I stopped there.  Some of you may be able \n",
    "# to do better than that, if you are willing to spend the time.  I have seen \n",
    "# references to CNN models achieving better than 90% accuracy on the CIFAR-10 data!\n",
    "\n",
    "# While your grade for this task does NOT depend on doing better than mine, you\n",
    "# should easily be able to do better than the baseline.  \n",
    "\n",
    "# REQUIRED: For each model that you try, add some comments to the notebook \n",
    "# that BRIEFLY describe the changes and what the final accuracy was for that model.\n",
    "\n",
    "# Make sure your final code is the model definition that gave you your best results.\n",
    "\n",
    "# Begin by copying the code for the baseline model into this cell from the previous task,\n",
    "# run it for only 2 epochs as suggested above, make a note of the results, and then start\n",
    "# making modified versions until you find something better.  \n",
    "\n",
    "# When you start making modifications, change the variable name of your model to\n",
    "# revisedCNN.  Do NOT continue to use the variable baseCNN\n",
    "\n",
    "# The more brief written summaries/results of models that I see in your comments,  \n",
    "# the more of the 30 points you will receive.  But you only need to show me your \n",
    "# final, best model, which I will also run on my own computer.\n",
    "\n",
    "# Have fun!!\n",
    "\n",
    "##########  Your Code Below ###############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########  Your Code Above ###############\n",
    "\n",
    "\n",
    "# One last warning:  CAREFULLY read my comments in this entire cell \n",
    "# to avoid errors.  ALSO, re-read the checklist in the markup cell \n",
    "# just above this cell to avoid missing something important!!!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
