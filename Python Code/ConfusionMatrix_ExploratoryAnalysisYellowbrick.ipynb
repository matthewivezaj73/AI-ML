{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bad47f1-c15b-4b32-b38c-7c6ecd11d9c5",
   "metadata": {},
   "source": [
    "# Assignment 4: 75 points (+ 10 extra credit for SVM kernel trick)\n",
    "## Support Vector Classifier, L1 & L2 Regularization, more on Cross Validation.\n",
    "\n",
    "### IMPORTANT: \n",
    "#### You MUST read everything in this notebook CAREFULLY, including ALL code comments.  If you do not, then you may easily make mistakes.\n",
    "\n",
    "This week we will build a Support Vector Machine as described in our textbook, and evaluate its expected performance using cross-validation, but we will not be combining cross-validation with hyperparameter tuning this time.\n",
    "\n",
    "Be sure to review the class slides if you need to. (But read the comments in this notebook first.)\n",
    "\n",
    "You may need to consult the following documentation URLs in order to complete this assignment:\n",
    "\n",
    "Documentation for LinearSVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "\n",
    "Documentation for cross validation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "\n",
    "### Important note:\n",
    "\n",
    "The original version of this notebook used a package called 'beepy', but that package will not work if you are using a Python version higher than 3.7.  Thus, the notebook was updated to use a package called 'playsound'. It will be used in Task 7 and the optional Task 8. Instructions for installation and the downloadable audio file were included in Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f06b2-6156-41b5-aaa1-52958fc350e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: 5 points.  Set up environment\n",
    "\n",
    "# If some of these do not import properly, you may need to install them and re-run\n",
    "\n",
    "import keras\n",
    "import playsound\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import time\n",
    "    \n",
    "import matplotlib         as mpl   \n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy              as np   \n",
    "import pandas             as pd\n",
    "\n",
    "from keras.datasets          import cifar10 \n",
    "from playsound               import playsound\n",
    "from pprint                  import pprint   \n",
    "from sklearn.linear_model    import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics         import confusion_matrix, precision_recall_curve, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline        import make_pipeline\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.svm             import LinearSVC, SVC\n",
    "from yellowbrick.classifier  import ClassBalance, ClassificationReport, ClassPredictionError, ConfusionMatrix, ROCAUC\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2470c9-21c3-4bd1-a14a-71db521d1b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 2: 5 points \n",
    "\n",
    "####################################################################################################\n",
    "#### Add your code below to load and preprocess the CIFAR-10 dataset as in previous assignments ####\n",
    "####           including a random seed of 42, all training data, test data and LABEL_NAMES      ####\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc21a76-a04a-4ee3-b9fa-36627265d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3, 10 Points\n",
    "\n",
    "# You will be studying the support vector machine (SVM) this week.\n",
    "# which is implemented by LinearSVC\n",
    "\n",
    "# Last week when you implemented a logistic regression classifier\n",
    "# you noticed that it ran quite slowly, and you will again\n",
    "# discover that LinearSVC is slow for the same reason.  This is\n",
    "# because both algorithms implement a binary classifier, i.e. they can only \n",
    "# recognize 2 categories.  They deal with the ten categories of \n",
    "# the CIFAR dataset by using a one-vs-others approach. \n",
    "# Thus, both logistic regression and LinearSVC build TEN models, \n",
    "# each recognizing only one of the 10 CIFAR-10 categories, e.g. the 'bird'\n",
    "# classifier will recognize pictures of birds vs. everything else.\n",
    "\n",
    "# from sklearn.svm           import LinearSVC\n",
    "# from sklearn.pipeline      import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# It is highly recommended to normalize input data to SVC, usually with\n",
    "# sklearn's StandardScaler (which creates data that has a mean of 0)\n",
    "# But we have already normalized our CIFAR data using the max-min method\n",
    "# so it ranges between 0 and 1.  If you standardize that to center it at 0\n",
    "# it will actually do slightly worse than just keeping it normalized as is.\n",
    "\n",
    "# n_jobs is not supported by LinearSVC, so you will have to wait a bit\n",
    "# for this, more than 10 minutes on my computer.\n",
    "\n",
    "# 4 points: Create a LinearSVC model with 'random_state' of 42\n",
    "#           and 'max_iter' to only 100, which will stop way too early, \n",
    "#           likely generate a warning, but will save you a LOT of wait time!\n",
    "# 4 points: Call its fit method to train the model on X_train_flat, y_train\n",
    "# 2 points: Print the accuracy score of svcModel with an appropriate message \n",
    "\n",
    "startTime  = time.perf_counter() \n",
    "####################  insert your code below for 10 points ####################\n",
    "\n",
    "svcModel = \n",
    "\n",
    "\n",
    "\n",
    "##########################  insert your code above ############################\n",
    "stopTime   = time.perf_counter()                                    \n",
    "print(f'\\nElapsed time: {stopTime - startTime:0.0f} seconds')\n",
    "playsound('yourcodeisdonerunning.m4a') \n",
    "\n",
    "# You will see a score that is much worse than the approx 40% accuracy with\n",
    "# logistic regression, but that's ok for now.  We are just practicing building\n",
    "# different models.  We will do better than 40% later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-driving",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 4: 10 Points\n",
    "\n",
    "# CROSS VALIDATION Revisited\n",
    "\n",
    "# Previously we performed grid search using cross validation  \n",
    "# for hyperparameter tuning via the GridSearchCV class.\n",
    "# However, we can run cross validation with cross_val_score\n",
    "# without tuning any hyperparameters to get a more precise estimate of its performance.\n",
    "\n",
    "# Here is the documentation to get the correct parameter names for cross_val_score:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "# 2 points:  Define svcModel the same way as in the previous cell.\n",
    "# 8 points:  Call cross_val_score on svcModel and the training data, using 3-fold \n",
    "#            cross validation and n_jobs = -1 (both will save time), \n",
    "#            'accuracy' for scoring, and set 'verbose' to True which will \n",
    "#            print the elapsed time without you computing it yourself.\n",
    "\n",
    "####################  insert your code below for 10 points ####################\n",
    "\n",
    "svcModel   =  \n",
    "xValScores = \n",
    "\n",
    "##########################  insert your code above ##########################\n",
    "\n",
    "print('\\nCross validation scores are:\\t', xValScores, '\\nTheir mean is:\\t\\t\\t', \n",
    "      xValScores.mean(), '\\nTheir standard deviation is:\\t', xValScores.std())\n",
    "\n",
    "# On my laptop that ran in LESS time than the previous cell\n",
    "# where we trained the model only once compared to 3 times here.  \n",
    "# This is because the training used only 2/3 of the data \n",
    "# for each of the 3 models and because all 8 of my laptop's cores\n",
    "# were used because of n_jobs=-1. If you have fewer cores, it will\n",
    "# likely take longer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57581ef4-7762-4a3d-a8ce-7f5847a62374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 5, 10 Points\n",
    "\n",
    "# Investigate both L1 and L2 Regularization\n",
    "\n",
    "# You may need to consult the documentation at:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "# We will build several models with different values for alpha, which\n",
    "# controls the strength of the regularization. From chapter 4 and my lectures,\n",
    "# you will learn that if alpha is large enough, L1 regularization will force\n",
    "# some of the variable coefficients to zero, which might be helpful\n",
    "# if their associated input variables (the pixel values for our CIFAR-10 data)\n",
    "# are not very useful for predictions.\n",
    "# This cannot happen with L2 regularization, but L2's advantage is when\n",
    "# most or all variables are helpful in making predictions.\n",
    "\n",
    "# We will switch back to SGDClassifier because it runs MUCH faster than\n",
    "# LinearSVC or logistic regression.  Here are some of the parameters to use:\n",
    "# loss = 'modified_huber', n_jobs = -1, random_state = 42\n",
    "\n",
    "# When alpha = 0, no regularization is used at all. \n",
    "# However, if alpha = 0 an error message will occur because alpha \n",
    "# is also used to affect the learning rate. So to avoid that error,\n",
    "# we will set alpha to a VERY small number, effectively \n",
    "# eliminating any effect of regularization. We will therefore use:\n",
    "# alpha=0.0000000000000001\n",
    "# If you get an error using that value, delete one of the zeros and try again (and again..)\n",
    "\n",
    "# 8 points: Set baselineSGD to an SGDClassifier with the parameters just described.\n",
    "# 2 points: Call the fit method of baselineSGD on the training data.\n",
    "\n",
    "####################  insert your code below for 10 points ####################\n",
    "\n",
    "baselineSGD = \n",
    "\n",
    "\n",
    "\n",
    "###########################  insert your code above ##########################\n",
    "\n",
    "print('\\nFor alpha = 0.0000000000000001,')\n",
    "print(' Baseline score on test data is:\\n', baselineSGD.score(X_test_flat, y_test))   \n",
    "print(' Baseline coefficients are:\\n') \n",
    "pprint(baselineSGD.coef_)   # Look at the learned coefficients\n",
    "                            # You should not see any coefficient with value of 0.0\n",
    "    \n",
    "# When I did this, I had a baseline accuracy score of 0.273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081c34b-cfb9-4e4a-a614-f38b1b90bb36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 6: 10 Points   \n",
    "\n",
    "# You may need to consult the documentation at:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "# Now we will compare that baseine to models that use either L1\n",
    "# or L2 regularization, to help reduce overfitting. We will\n",
    "# look at the coefficients because we want to see if any of them are forced\n",
    "# to have zero values when we use L1, assuming that alpha is large enough\n",
    "# for that to happen.\n",
    "\n",
    "# Define and train an L1 regularized SGDClassifier and \n",
    "# another one with L2 and display the accuracy scores of both \n",
    "# to compare their performance. Use 0.0008 for the value of alpha.  \n",
    "# Use 'pprint' (NOT 'print) to print the values of the model coefficients.\n",
    "\n",
    "# For loss, n_jobs and random_state use the same values as the previous cell.\n",
    "\n",
    "# To build a classifier with L1 use: \n",
    "# penalty='l1'\n",
    "# and for L2 use:\n",
    "# penalty='l2'\n",
    "\n",
    "# On my computer this takes less than 3 minutes.\n",
    "\n",
    "# 5 points: Set l1SGD to the SGDClassifier described above using L1 penalty\n",
    "#           Call its fit method to train it.\n",
    "#           Print the trained model's score with some appropriate message.\n",
    "#           Use 'print' to print some appropriate message about the coefficients.\n",
    "#           Use 'pprint' to print those model coefficients. To access the\n",
    "#           coefficients reference the 'coef_' property of the trained model,\n",
    "#           which you can see in the previous cell.\n",
    "#\n",
    "# 5 points: Do the same thing again, but using l2SGD for L2 penalty\n",
    "\n",
    "####################  insert your code below for 10 points ####################  \n",
    "\n",
    "print('\\nFor alpha = 0.0008') # Now enter the rest of the code yourself, above description\n",
    "\n",
    "l1SGD = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "l2SGD = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################  insert your code above ##########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567341d1-7a9e-4cdb-ab16-8e45d5bf69b6",
   "metadata": {},
   "source": [
    "#### What do you see in the output?  \n",
    "You should be able to see that many of the coefficients for the L1 model were squeezed down to 0, which means their associated pixels will be ignored.\n",
    "\n",
    "On my computer I got better accuracy with the L1 model, suggesting that the L1 assumption of pixel importance is true.  However, there are other parameter settings I was able to find such that L2 always does better than L1, which is generally the case for most models, and explains why L2 is the default regularization technique for SGDClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140d9d9-945a-4cc4-827b-f4cddcd933d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7, 25 Points  \n",
    "\n",
    "# You may need to consult the documentation at:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "# Elasticnet Regularization is when you use BOTH L1 and L2 in your model.\n",
    "# For Elasticnet there is another parameter called l1_ratio in sklearn\n",
    "# that ranges between 0 and 1 and controls the relative strength of \n",
    "# L1 vs. L2. When l1_ratio = 0, then L1 is ignored and the model only uses \n",
    "# L2, and when l1_ratio = 1, then L2 is ignored and it's only using L1.\n",
    "\n",
    "# So let's see if we can do better than the previous cell by \n",
    "# using elasticnet.  In my own experimentation, I found that\n",
    "# with the models we have been using here, L1 does best with\n",
    "# alpha values near 0.0008 and L2 does best around 0.0005.\n",
    "# So let's try several different values for l1_ratio all \n",
    "# with alpha=0.00065, which is the midpoint for 0.0005 and 0.0008\n",
    "\n",
    "# Write a python for-loop to:\n",
    "# 5 points: loop through the values [0.25, 0.33, 0.5, 0.67, 0.75] \n",
    "#           with loop variable L1Ratio\n",
    "# 5 points: create an SGDClassifier using a penalty of 'elasticnet', \n",
    "#           alpha of 0.00065 for each model, l1_ratio set to the loop variable,\n",
    "#           and the same values for loss, n_jobs, and random_state as before.\n",
    "# 5 points: fit the model on the training data\n",
    "# 5 points: print messages indicating the value of L1Ratio and the model's score\n",
    "# 5 points: print messages about the elapsed time, and call playsound\n",
    "\n",
    "# Also add code to print the elapsed time information and notify yourself with playsound\n",
    "\n",
    "####################  insert your code below for 25 points ####################\n",
    "\n",
    "\n",
    "print('For alpha = 0.00065') # Add the rest of the code yourself, per instructions above\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################  insert your code above ##########################\n",
    "\n",
    "# Notice how the accuracy fluctuates with each\n",
    "# increasing value of L1Ratio through the loop.\n",
    "# Hyperparameter tuning would be easier if it was more predictable!\n",
    "# But the reason I did not ask you to do hyperparameter tuning here\n",
    "# is simply to save time during training, but this will still be\n",
    "# relatively slow, about 6 minutes on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e0f413-9077-4aae-90bb-c075b97950c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8: 10 extra credit points\n",
    "\n",
    "# Polynomial SVC with Kernel Trick\n",
    "\n",
    "# If you choose to do this optional task, you will be using\n",
    "# a different support vector machine algorithm called 'SVC'.\n",
    "\n",
    "# You will need to consult the documentation for SVC at:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html \n",
    "# for the appropriate parameter names to use as arguments.\n",
    "\n",
    "# 8 points: Define a a polynomial support vector classifier of degree 5\n",
    "#           Set coef0 to 5, C to 5 and max_iter to only 200 (to save time again)\n",
    "# 2 points: Train the model \n",
    "\n",
    "# About 5 minutes on my computer\n",
    "# Note: After you see the elapsed time message you will\n",
    "#       STILL have to wait before you see the prediction score.\n",
    " \n",
    "startTime   = time.perf_counter() \n",
    "\n",
    "####################  insert your code below for 10 points ####################\n",
    "\n",
    "svcModel    = \n",
    "\n",
    "\n",
    "###########################  insert your code above ###########################\n",
    "\n",
    "stopTime    = time.perf_counter()              \n",
    "print(f'\\nElapsed time:\\t {stopTime - startTime:0.4f} seconds')  \n",
    "\n",
    "print('SVC Score with Kernel Trick:', svcModel.score(X_test_flat, y_test))  # Even this slow, so be patient!\n",
    "playsound('yourcodeisdonerunning.m4a')\n",
    "\n",
    "# You will see that this does not do very well as a classifier on this dataset\n",
    "# but we could certainly do better if we tune several of the hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
